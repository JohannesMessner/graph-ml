{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "established-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as geom_nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.utils import from_networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-passage",
   "metadata": {},
   "source": [
    "# Generating a cycle dataset\n",
    "\n",
    "Here we generate a dataset containing pairs of graphs that are not distinguishable by the 1-WL isomorphism test.\n",
    "Later we will use GNNs to learn to tell them apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retired-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_nx = []\n",
    "graphs_is_cycle = []  # keeps track if a graph is a cycle or disjoint\n",
    "for n in range(6, 16):\n",
    "    g_cyc = nx.Graph()\n",
    "    g_cyc.add_nodes_from(range(n))\n",
    "    g_cyc.add_edges_from([(x, x+1) for x in range(n-1)] + [(n-1, 0)])  # connect nodes to cycle\n",
    "    \n",
    "    for split_n in range(3,n-2):\n",
    "        g_split = nx.Graph()\n",
    "        g_cyc.add_nodes_from(range(n))\n",
    "        g_split.add_edges_from([(x, x+1) for x in range(split_n-1)] + [(split_n-1, 0)])  # first cycle of size split_n\n",
    "        g_split.add_edges_from([(x, x+1) for x in range(split_n, n-1)] + [(n-1, split_n)])  # dsecond cycle of remaing nodes\n",
    "        graphs_nx.append(g_split)\n",
    "        graphs_is_cycle.append(False)\n",
    "        graphs_nx.append(g_cyc)  # add g_cyc every time to maintain balance\n",
    "        graphs_is_cycle.append(True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interior-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the graphs to torch_geometric.data.Data objects\n",
    "graphs = [from_networkx(g) for g in graphs_nx]\n",
    "for i_g, g in enumerate(graphs):\n",
    "    g.x = torch.zeros((g.num_nodes, 50))  # uniform x/features\n",
    "    g.y = torch.tensor([1 if graphs_is_cycle[i_g] else 0])  # target label indicating whether graph is cycle or disjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-flesh",
   "metadata": {},
   "source": [
    "# Building a GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "meaning-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom message passing layer\n",
    "#class CustomLayer(geom_nn.MessagePassing):\n",
    "    #def __init__(self, in_channels, out_channels):\n",
    "        #super().__init__(aggr='add')\n",
    "        #self.lin = nn.Linear(in_channels, out_channels)\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "    # activation, linear etc probably goes here\n",
    "    #def forward(self, x, edge_index):\n",
    "        #return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    # stuff that happens after all the message passing, so just id??\n",
    "    #def update(self, x):\n",
    "        #return self.activation(self.linear(x))\n",
    "        \n",
    "    # this looks fine\n",
    "    #def message(self, x_j):\n",
    "        #return x_j\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, depth, mp_layer=None):\n",
    "        super().__init__()\n",
    "        # the type of message passing layer used throughout the net\n",
    "        self.mp_layer = geom_nn.GCNConv if mp_layer is None else mp_layer\n",
    "        self.pool = geom_nn.global_mean_pool\n",
    "        self.mp_layers = nn.ModuleList()\n",
    "        \n",
    "        # standard mlp used after message passing layers\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        # add message passing layers\n",
    "        self.mp_layers.append(self.mp_layer(input_dim, hidden_dim))\n",
    "        for i in range(depth-1):\n",
    "            self.mp_layers.append(self.mp_layer(hidden_dim, hidden_dim))\n",
    "            \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for l in self.mp_layers:\n",
    "            x = l(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        x = self.pool(x, batch)\n",
    "        x = self.post_mp(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, pred, label):\n",
    "        return torch.nn.functional.binary_cross_entropy(pred, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "digital-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, nb_epochs=50):\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    #train_loader = DataLoader(dataset[:int(0.8*len(graphs))], batch_size=32, shuffle=False)\n",
    "    #test_loader = DataLoader(dataset[int(0.2*len(graphs)):], batch_size=32, shuffle=True)\n",
    "    \n",
    "    model = Net(dataset[0].num_node_features, 50, 1, 16)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    for epoch in range(nb_epochs):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch).flatten()\n",
    "            label = batch.y.to(torch.float32)\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        #if epoch % 10 == 0:\n",
    "            #test_acc = test(model, test_loader)\n",
    "            #print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(epoch, total_loss, test_acc))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "satellite-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset):\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    tot = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            pred = model(data).flatten()\n",
    "            label = data.y\n",
    "            pred = [0 if p <= 0.5 else 1 for p in pred]  # 0 if prediction <= 0.5, 1 othwerwise\n",
    "            correct_i = np.equal(pred, label)\n",
    "            correct += np.array(correct_i).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "vulnerable-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(data, k=5, nb_epochs=200):\n",
    "    # We perform k-fold cross validation\n",
    "    chunks = []\n",
    "    chunk_size = int(len(data)/k)\n",
    "    # split the data\n",
    "    for i in range(k):\n",
    "        if i*chunk_size+chunk_size <= len(data):\n",
    "            chunks.append(data[i*chunk_size:k+chunk_size])\n",
    "        else:\n",
    "            chunks.append(data[i*chunk_size:])\n",
    "    # perform training and testing\n",
    "    accuracies = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        test_set = chunk\n",
    "        train_set = [x for l in (chunks[0:i] + chunks[i+1:]) for x in l]\n",
    "        #train_set = np.concatenate((np.array(chunks[0:i]).flatten(), np.array(chunks[i+1:]).flatten()))\n",
    "        #print(train_set)\n",
    "        model = train(train_set, nb_epochs=nb_epochs)\n",
    "        acc = test(model, train_set)\n",
    "        accuracies.append(acc)\n",
    "        print('Accuracy at test-chunk {}: {}'.format(i, acc))\n",
    "    return np.array(accuracies).sum() / len(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ranking-tiger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at test-chunk 0: 0.6\n",
      "Accuracy at test-chunk 1: 0.5185185185185185\n",
      "Accuracy at test-chunk 2: 0.53125\n",
      "Accuracy at test-chunk 3: 0.53125\n",
      "Accuracy at test-chunk 4: 0.53125\n",
      "0.5424537037037037\n"
     ]
    }
   ],
   "source": [
    "# we exprect a test accuracy of 0.5, since these types of graphs can't be distinguished by the net\n",
    "avg_acc = cross_validate(graphs)\n",
    "print(avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-simon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
